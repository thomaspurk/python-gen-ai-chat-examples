{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyN0G6tHDpJn+amtU6QBcZay",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thomaspurk/python-gen-ai-chat-examples/blob/main/Hugging_Face_Agents_Course_Dummy_Agent_Library.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Notebook: Hugging Face Agents Course Dummy Agent Library\n",
        "# Author: Thomas Purk\n",
        "# Date: 2025-04-18\n",
        "# Reference: https://huggingface.co/learn/agents-course/unit1/introduction\n",
        "# Reference: https://huggingface.co/learn/agents-course/unit1/dummy-agent-library"
      ],
      "metadata": {
        "id": "liDJPVN_RWt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hugging Face Agents Course Dummy Agent Library\n",
        "\n",
        "This notebook is intended to serve as an alternative to the Colab notebook\n",
        "provided by the Hugging Face Agents course.\n",
        "\n",
        "https://huggingface.co/learn/agents-course/unit1/dummy-agent-library\n",
        "\n",
        "**Reason for the alternative**\n",
        "\n",
        "The course notebook cannot be ran using an InferenceClient object if the\n",
        "suggested mode \"meta-llama/Llama-3.2-3B-Instruct\" is not available.\n",
        "\n",
        "When I attempted to run the original notebook, I got the error...\n",
        "\n",
        "> HfHubHTTPError: 503 Server Error: Service Temporarily Unavailable for url: https://router.huggingface.co/hf-inference/models/meta-llama/Llama-3.2-3B-Instruct\n",
        "\n",
        "The alternative public end point \"https://jc26mwg228mkj8dw.us-east-1.aws.endpoints.huggingface.cloud\" also gave me an error...\n",
        "\n",
        "> BadRequestError: (Request ID: OUr7-S) Bad request: Bad Request: Invalid state\n",
        "\n",
        "So, for me and others on the Discord channel, it was not possible to run the\n",
        "original course notebook. Hopefully, others can run this notebook to experience\n",
        "the intended lessons of the original notebook.\n",
        "\n",
        "**Changes in this Notebook**\n",
        "\n",
        "- Using the smaller \"meta-llama/Llama-3.2-1B-Instruct\" model\n",
        "- Using Transformer Pipelines instead of InferenceClient\n",
        "- Using the chat template from the model tokenizer (as discussed earlier in Unit 1)\n",
        "- Wrapping get_weather using the @tool decoractor and matching dummy class (as discussed earlier in Unit 1)\n",
        "- The course notebook uses two text_generation calls to InferenceClient to simulate the dummy agent behavior. I uses the \"stop\" parameter to stop at \"Observation:\" on the first call, compiles the result into a second prompt to finish the agent action.\n",
        "- This notebook breaks the text generation into three phases instead of two, one for each of Thought, Action, Observation.\n",
        "- This notebook gets a JSON object from the Action phase, which is parsed into a dict. The keys/values of the dict are used to execute the get_weather function. A tiny bit closer to an actual agent.\n",
        "\n",
        "**Environment**\n",
        "\n",
        "- Google Colab (I have Colab Pro)\n",
        "- CPU runtime (Runs in a few minutes, GPU not required.)\n",
        "- Tokens stored in Colab Secrets"
      ],
      "metadata": {
        "id": "JYBuOMR4Ry6D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Notebook Setup\n",
        "- Imports\n",
        "- Custom Functions"
      ],
      "metadata": {
        "id": "bP7LEBsvIrLs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "XH0vAN0PZAj8"
      },
      "outputs": [],
      "source": [
        "# Set up Notebook\n",
        "\n",
        "import os\n",
        "import inspect\n",
        "import json\n",
        "import re\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "from IPython.display import Markdown\n",
        "\n",
        "# Use Hugging Face Pipeline instead of InferenceClient\n",
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Get the Google Colab secret and add to the evironment variables\n",
        "# Update for other environments as needed.\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN_2')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dummy Tool Class, Decorator, & Function\n",
        "# From \"What are Tools?\" course section\n",
        "\n",
        "class Tool:\n",
        "    \"\"\"\n",
        "    A class representing a reusable piece of code (Tool).\n",
        "\n",
        "    Attributes:\n",
        "        name (str): Name of the tool.\n",
        "        description (str): A textual description of what the tool does.\n",
        "        func (callable): The function this tool wraps.\n",
        "        arguments (list): A list of argument.\n",
        "        outputs (str or list): The return type(s) of the wrapped function.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 name: str,\n",
        "                 description: str,\n",
        "                 func: callable,\n",
        "                 arguments: list,\n",
        "                 outputs: str):\n",
        "        self.name = name\n",
        "        self.description = description\n",
        "        self.func = func\n",
        "        self.arguments = arguments\n",
        "        self.outputs = outputs\n",
        "\n",
        "    def to_string(self) -> str:\n",
        "        \"\"\"\n",
        "        Return a string representation of the tool,\n",
        "        including its name, description, arguments, and outputs.\n",
        "        \"\"\"\n",
        "        args_str = \", \".join([\n",
        "            f\"{arg_name}: {arg_type}\" for arg_name, arg_type in self.arguments\n",
        "        ])\n",
        "\n",
        "        return (\n",
        "            f\"Tool Name: {self.name},\"\n",
        "            f\" Description: {self.description},\"\n",
        "            f\" Arguments: {args_str},\"\n",
        "            f\" Outputs: {self.outputs}\"\n",
        "        )\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        \"\"\"\n",
        "        Invoke the underlying function (callable) with provided arguments.\n",
        "        \"\"\"\n",
        "        return self.func(*args, **kwargs)\n",
        "\n",
        "def tool(func):\n",
        "    \"\"\"\n",
        "    A decorator that creates a Tool instance from the given function.\n",
        "    \"\"\"\n",
        "    # Get the function signature\n",
        "    signature = inspect.signature(func)\n",
        "\n",
        "    # Extract (param_name, param_annotation) pairs for inputs\n",
        "    arguments = []\n",
        "    for param in signature.parameters.values():\n",
        "        annotation_name = (\n",
        "            param.annotation.__name__\n",
        "            if hasattr(param.annotation, '__name__')\n",
        "            else str(param.annotation)\n",
        "        )\n",
        "        arguments.append((param.name, annotation_name))\n",
        "\n",
        "    # Determine the return annotation\n",
        "    return_annotation = signature.return_annotation\n",
        "    if return_annotation is inspect._empty:\n",
        "        outputs = \"No return annotation\"\n",
        "    else:\n",
        "        outputs = (\n",
        "            return_annotation.__name__\n",
        "            if hasattr(return_annotation, '__name__')\n",
        "            else str(return_annotation)\n",
        "        )\n",
        "\n",
        "    # Use the function's docstring as the description (default if None)\n",
        "    description = func.__doc__ or \"No description provided.\"\n",
        "\n",
        "    # The function name becomes the Tool name\n",
        "    name = func.__name__\n",
        "\n",
        "    # Return a new Tool instance\n",
        "    return Tool(\n",
        "        name=name,\n",
        "        description=description,\n",
        "        func=func,\n",
        "        arguments=arguments,\n",
        "        outputs=outputs\n",
        "    )\n",
        "\n",
        "# NOTE: Divided the docstring for get_weather, so that the to_string\n",
        "# result would not include \"Parameters\" & \"Returns.\"\n",
        "@tool\n",
        "def get_weather(location: str) -> str:\n",
        "    \"\"\"Get the current weather in a given the name of a city.\"\"\"\n",
        "    \"\"\"\n",
        "        Parameters:\n",
        "            location (str): The location of interest.\n",
        "\n",
        "        Returns:\n",
        "            str: A short descirption of the whether.\n",
        "\n",
        "    \"\"\"\n",
        "    return f\"the weather in {location} is sunny with low temperatures.\"\n",
        "\n",
        "# Validate getting the string description\n",
        "print(get_weather.to_string())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xc4gRhHnmIs",
        "outputId": "184205f7-6a22-4893-c127-915f419ce594"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tool Name: get_weather, Description: Get the current weather in a given the name of a city., Arguments: location: str, Outputs: str\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate get_weather\n",
        "print(type(get_weather))\n",
        "print(get_weather('Chicago'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHUNzikN6krD",
        "outputId": "4443aff5-ec71-4428-c17b-971b978f5086"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class '__main__.Tool'>\n",
            "the weather in Chicago is sunny with low temperatures.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup the model\n",
        "checkpoint = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "# \"Instruct\" models are trained for agent actions and typically provide a chat template.\n",
        "# Validate chat template exists\n",
        "Markdown(f'Chat Template:\\n\\n {tokenizer.chat_template}')"
      ],
      "metadata": {
        "id": "OZwJtG8Ff0F9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "outputId": "19c5140b-e9ae-4be7-faf4-e16daa5e0c0a"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Chat Template:\n\n {{- bos_token }}\n{%- if custom_tools is defined %}\n    {%- set tools = custom_tools %}\n{%- endif %}\n{%- if not tools_in_user_message is defined %}\n    {%- set tools_in_user_message = true %}\n{%- endif %}\n{%- if not date_string is defined %}\n    {%- if strftime_now is defined %}\n        {%- set date_string = strftime_now(\"%d %b %Y\") %}\n    {%- else %}\n        {%- set date_string = \"26 Jul 2024\" %}\n    {%- endif %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content']|trim %}\n    {%- set messages = messages[1:] %}\n{%- else %}\n    {%- set system_message = \"\" %}\n{%- endif %}\n\n{#- System message #}\n{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n{%- if tools is not none %}\n    {{- \"Environment: ipython\\n\" }}\n{%- endif %}\n{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n{%- if tools is not none and not tools_in_user_message %}\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n{%- endif %}\n{{- system_message }}\n{{- \"<|eot_id|>\" }}\n\n{#- Custom tools are passed in a user message with some extra guidance #}\n{%- if tools_in_user_message and not tools is none %}\n    {#- Extract the first user message so we can plug it in here #}\n    {%- if messages | length != 0 %}\n        {%- set first_user_message = messages[0]['content']|trim %}\n        {%- set messages = messages[1:] %}\n    {%- else %}\n        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n{%- endif %}\n    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n    {{- first_user_message + \"<|eot_id|>\"}}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n    {%- elif 'tool_calls' in message %}\n        {%- if not message.tool_calls|length == 1 %}\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n        {%- endif %}\n        {%- set tool_call = message.tool_calls[0].function %}\n        {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n        {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n        {{- '\"parameters\": ' }}\n        {{- tool_call.arguments | tojson }}\n        {{- \"}\" }}\n        {{- \"<|eot_id|>\" }}\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n        {%- if message.content is mapping or message.content is iterable %}\n            {{- message.content | tojson }}\n        {%- else %}\n            {{- message.content }}\n        {%- endif %}\n        {{- \"<|eot_id|>\" }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}\n"
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Text Generation Pipeline\n",
        "tg_pipeline = pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=checkpoint\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dVGCo08d46E",
        "outputId": "ff33a0d3-ebbb-428a-b3fc-195985c26f0d"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Manually Build and Execute the Dummy Agent"
      ],
      "metadata": {
        "id": "F1m2vpTFDitv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example question the user might ask the agent\n",
        "user_question = \"What's the weather in London?\""
      ],
      "metadata": {
        "id": "TvYL3loe-JcK"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: Pipelines do not have a \"stop\" parameter similar to the InferenceClient shown\n",
        "# in the course tutorial.\n",
        "\n",
        "# So, break up the prompt into Thought -> Action > Observation cycle phase"
      ],
      "metadata": {
        "id": "smHezhQV65ns"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Thought Phase Messages\n",
        "\n",
        "THOUGHT_PHASE = f\"\"\"You are given the user question '{user_question}.' Do not\n",
        "answer the question. Instead think about the action you need to take in\n",
        "order to determine the answer to this question given that you are provided\n",
        "access to the following tools:\n",
        "\n",
        "{get_weather.to_string()}\n",
        "\n",
        "Provide a response that summarizes the conclusion resulting from you thoughts in\n",
        " a single concise sentence.\"\"\"\n",
        "\n",
        "# Create the Thought Phase Messages list\n",
        "thought_messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You think about how to answer questions instead of providing answers.\"  },\n",
        "    {\"role\": \"user\", \"content\": THOUGHT_PHASE},\n",
        "]\n",
        "\n",
        "# Though Phase Prompt - Apply the chat template\n",
        "thought_prompt = tokenizer.apply_chat_template(\n",
        "    conversation=thought_messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "\n",
        "# Display / Validate\n",
        "print(thought_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCxY7mPP7JDV",
        "outputId": "b077e076-e1b2-4e42-a818-b90afb34fc01"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 19 Apr 2025\n",
            "\n",
            "You think about how to answer questions instead of providing answers.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "You are given the user question 'What's the weather in London?.' Do not \n",
            "answer the question. Instead think about the action you need to take in\n",
            "order to determine theanswer this question given that you are provided access to the following tools:\n",
            "\n",
            "Tool Name: get_weather, Description: Get the current weather in a given the name of a city., Arguments: location: str, Outputs: str\n",
            "\n",
            "Provide a response that summarizes the conclusion resulting from you thoughts in\n",
            " a single concise sentence.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute the pipeline to generate an LLM response\n",
        "thought_output = tg_pipeline(\n",
        "    thought_prompt,\n",
        "    max_new_tokens=200,\n",
        "    pad_token_id=128001\n",
        ")\n",
        "\n",
        "# Format the result to remove the original prompt text\n",
        "thought_output = thought_output[0]['generated_text'].replace(thought_prompt,'')\n",
        "thought_output = f\"Thought: {thought_output}\""
      ],
      "metadata": {
        "id": "LMGVJxR329nU"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate Tought Output\n",
        "print(thought_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c8vXVnT3DoV",
        "outputId": "4e894270-672f-445d-dcb2-88693a870584"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thought: To answer this question, I need to determine the location of London and then use the `get_weather` tool to retrieve the current weather for that location.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Action Phase Messages\n",
        "\n",
        "ACTION_PHASE = f\"\"\"You are provided access to the following tools:\n",
        "\n",
        "{get_weather.to_string()}\n",
        "\n",
        "Create a JSON object that describes action instructions. You must specify\n",
        "a JSON object containing an 'action' key that is the name of the tool and an\n",
        "'action_input' key that is the input parameters to the tool.\n",
        "\n",
        "Respond only with a JSON object and no other text.\n",
        "\n",
        "example response:\n",
        "\n",
        "{{\n",
        "  'action': 'get_weather',\n",
        "  'action_input': {{'location': 'New York'}}\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "# Create the Messages list\n",
        "action_messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You create JSON objects containing tool instructions\"  },\n",
        "    {\"role\": \"user\", \"content\":  thought_output + \"\\n\\n\" + ACTION_PHASE}\n",
        "]\n",
        "\n",
        "# Action Phase Prompt - Apply the chat template\n",
        "action_prompt = tokenizer.apply_chat_template(\n",
        "    conversation=action_messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "\n",
        "# Display / Validate\n",
        "print(action_prompt)\n"
      ],
      "metadata": {
        "id": "BmTJsP3dekCs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5821a56c-2561-4200-c650-fbd1593d05c7"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 19 Apr 2025\n",
            "\n",
            "You create JSON objects containing tool instructions<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Thought: To answer this question, I need to determine the location of London and then use the `get_weather` tool to retrieve the current weather for that location.\n",
            "\n",
            "You are provided access to the following tools:\n",
            "\n",
            "Tool Name: get_weather, Description: Get the current weather in a given the name of a city., Arguments: location: str, Outputs: str\n",
            "\n",
            "Create a JSON object that describes action instructions. You must specify\n",
            "a JSON object containing an 'action' key that is the name of the tool and an\n",
            "'action_input' key that is the input parameters to the tool.\n",
            "\n",
            "Respond only with a JSON object and no other text.\n",
            "\n",
            "example response:\n",
            "\n",
            "{\n",
            "  'action': 'get_weather',\n",
            "  'action_input': {'location': 'New York'}\n",
            "}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute the pipeline to generate an LLM response\n",
        "action_output = tg_pipeline(\n",
        "    action_prompt,\n",
        "    max_new_tokens=200,\n",
        "    pad_token_id=128001\n",
        ")\n",
        "\n",
        "# Format the result to remove the original prompt text\n",
        "action_output_raw = action_output[0]['generated_text'].replace(action_prompt,'')\n",
        "\n",
        "# Based on testing while coding, the model can sometimes add extra \"stuff\"\n",
        "# Keep only the JSON object text\n",
        "match = re.search(r'\\{.*\\}', action_output_raw, re.DOTALL)\n",
        "action_output_raw = match.group() # If this errors, we can't continue anyway.\n",
        "\n",
        "# More formatting\n",
        "action_output = f\"Action:\\n{action_output_raw}\""
      ],
      "metadata": {
        "id": "_ySARBqwilK7"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate Action Output\n",
        "print(action_output_raw)\n",
        "\n",
        "# If a valid JSON string is not the precise output, then the remaining cells\n",
        "# will error.\n",
        "\n",
        "# Fingers crossed for output similar to . . .\n",
        "# {\n",
        "#   \"action\": \"get_weather\",\n",
        "#   \"action_input\": {\"location\": \"London\"}\n",
        "# }"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvUkhuGBjiVT",
        "outputId": "4eae8944-3197-48ee-a7c1-6b161245230b"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"action\": \"get_weather\",\n",
            "  \"action_input\": {\n",
            "    \"location\": \"London\"\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute the Action\n",
        "\n",
        "# Attempt to parse the JSON string into a Python dict\n",
        "action_dict = json.loads(action_output_raw)\n",
        "\n",
        "# Get the action Name\n",
        "func_name = action_dict[\"action\"]\n",
        "\n",
        "# Get the function from locals\n",
        "action_result = locals()[func_name](**action_dict[\"action_input\"])\n",
        "\n",
        "# Validate Action Results\n",
        "print(f\"Action Result: {action_result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewSl7JrYAI9Y",
        "outputId": "d1ff6391-c4e0-487d-f713-2ab66323e523"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action Result: the weather in London is sunny with low temperatures.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Observation Phase Messages\n",
        "\n",
        "OBSERVATION_PHASE = f\"\"\"Given that the result of the action is {action_result},\n",
        "state your observation in the 'Observation' field within the following\n",
        "format.\n",
        "\n",
        "Observation: <your observation>\"\"\"\n",
        "\n",
        "# Build the final prompt\n",
        "final_prompt = thought_output + \"\\n\\n\" + \\\n",
        "    action_output + \"\\n\\n\" + \\\n",
        "    OBSERVATION_PHASE\n",
        "\n",
        "# Create the Messages list\n",
        "final_messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You comment on the results of an action.\"  },\n",
        "    {\"role\": \"user\", \"content\": final_prompt}\n",
        "]\n",
        "\n",
        "# Apply the chat template\n",
        "final_prompt = tokenizer.apply_chat_template(\n",
        "    conversation=final_messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "\n",
        "# Display / Validate\n",
        "print(final_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BY1J74Ff9e3G",
        "outputId": "3c48fc55-11d4-41d5-8fad-c27507c8b38e"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 19 Apr 2025\n",
            "\n",
            "You comment on the results of an action.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Thought: To answer this question, I need to determine the location of London and then use the `get_weather` tool to retrieve the current weather for that location.\n",
            "\n",
            "Action:\n",
            "{\n",
            "  \"action\": \"get_weather\",\n",
            "  \"action_input\": {\n",
            "    \"location\": \"London\"\n",
            "  }\n",
            "}\n",
            "\n",
            "Given that the result of the action is the weather in London is sunny with low temperatures.,\n",
            "return state your observation in the 'Observation' field within the following \n",
            "format.\n",
            "\n",
            "Observation: <your observation><|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute the pipeline to generate an LLM response\n",
        "final_output = tg_pipeline(\n",
        "    final_prompt,\n",
        "    max_new_tokens=200,\n",
        "    pad_token_id=128001\n",
        ")\n",
        "\n",
        "final_output = final_output[0]['generated_text'].replace(final_prompt,'')"
      ],
      "metadata": {
        "id": "3fg0ReDyqewI"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate Final Output\n",
        "print(final_output)"
      ],
      "metadata": {
        "id": "l8UJila_reBQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da074c34-c943-4ff9-92a6-dcbe0a88587c"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To determine the observation based on the given action and result, I'll follow these steps:\n",
            "\n",
            "1. Determine the location from the action input.\n",
            "2. Use the `get_weather` tool to retrieve the weather in that location.\n",
            "3. Analyze the result to determine the observation.\n",
            "\n",
            "Given the action input:\n",
            "```json\n",
            "{\n",
            "  \"action\": \"get_weather\",\n",
            "  \"action_input\": {\n",
            "    \"location\": \"London\"\n",
            "  }\n",
            "}\n",
            "```\n",
            "I'll assume that the `get_weather` tool returns a JSON response with the following structure:\n",
            "```json\n",
            "{\n",
            "  \"location\": \"London\",\n",
            "  \"weather\": {\n",
            "    \"temperature\": \"low\",\n",
            "    \"description\": \"sunny\"\n",
            "  }\n",
            "}\n",
            "```\n",
            "From this structure, I can determine that the observation is:\n",
            "\n",
            "Observation: The weather in London is sunny with low temperatures.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Conclusion: Not exectly the same output as the original, but good enough to\n",
        "# illustrate the learning points of the original"
      ],
      "metadata": {
        "id": "piGGXvKcRA6m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}